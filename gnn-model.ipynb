{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import Voronoi\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful functions\n",
    "\n",
    "def create_ffn(hidden_units, dropout_rate, input_shape=None, name=None):\n",
    "    \n",
    "    #Creates a sequential model (feed-forward network) \n",
    "   \n",
    "    fnn_layers = []\n",
    "    if input_shape is not None:\n",
    "        fnn_layers.append(layers.Input(shape=input_shape))\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "    return keras.Sequential(fnn_layers, name=name)\n",
    "\n",
    "def create_gru(hidden_units, dropout_rate):\n",
    "    \n",
    "    #Creates a GRU based model for combining nodes information\n",
    "    \n",
    "    inputs = keras.layers.Input(shape=(2, hidden_units[0]))\n",
    "    x = inputs\n",
    "    for units in hidden_units:\n",
    "        x = layers.GRU(\n",
    "            units=units,\n",
    "            activation=\"tanh\",\n",
    "            recurrent_activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate\n",
    "        )(x)\n",
    "    return keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "#Convolution layer\n",
    "\n",
    "class GraphConvLayer(layers.Layer):\n",
    "    def __init__(self, hidden_units, dropout_rate=0.2, aggregation_type=\"mean\",\n",
    "                 combination_type=\"concat\", normalize=False, *args, **kwargs):\n",
    "\n",
    "        # Layer that processes messages in a graph: prepares messages from neighbours with a FFN, \n",
    "        # aggregates messages of neighbours through a specified method (sum,mean,max) and combines \n",
    "        # the node representation with the aggregated message\n",
    "\n",
    "        super(GraphConvLayer, self).__init__(*args, **kwargs)\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # FFN para preparar mensajes\n",
    "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate, name=\"ffn_prepare\")\n",
    "        # Función de actualización: puede ser una GRU o una FFN\n",
    "        if self.combination_type == \"gru\":\n",
    "            self.update_fn = create_gru(hidden_units, dropout_rate)\n",
    "        else:\n",
    "            self.update_fn = create_ffn(hidden_units, dropout_rate, name=\"update_ffn\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #We can implement the variable initialization here if necessary \n",
    "        super(GraphConvLayer, self).build(input_shape)\n",
    "\n",
    "    def prepare(self, node_representations, weights=None):\n",
    "        messages = self.ffn_prepare(node_representations)\n",
    "        if weights is not None:\n",
    "            messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages, node_representations):\n",
    "        # As it can vary between images, we use the number of nodes dinamically\n",
    "        # node_indices shape is [num_edges].\n",
    "        # neighbour_messages shape: [num_edges, representation_dim].\n",
    "        # node_repesentations shape is [num_nodes, representation_dim]\n",
    "        num_nodes = tf.shape(node_representations)[0]\n",
    "        if self.aggregation_type == \"sum\":\n",
    "            aggregated_message = tf.math.unsorted_segment_sum(neighbour_messages, node_indices, num_segments=num_nodes)\n",
    "        elif self.aggregation_type == \"mean\":\n",
    "            aggregated_message = tf.math.unsorted_segment_mean(neighbour_messages, node_indices, num_segments=num_nodes)\n",
    "        elif self.aggregation_type == \"max\":\n",
    "            aggregated_message = tf.math.unsorted_segment_max(neighbour_messages, node_indices, num_segments=num_nodes)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_representations, aggregated_messages):\n",
    "        # node_repesentations shape is [num_nodes, representation_dim].\n",
    "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
    "        if self.combination_type == \"gru\":\n",
    "            h = tf.stack([node_representations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"concat\":\n",
    "            h = tf.concat([node_representations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            h = node_representations + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "        node_embeddings = self.update_fn(h)\n",
    "        if self.combination_type == \"gru\":\n",
    "            # Seleccionamos la salida final de la secuencia GRU\n",
    "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
    "        if self.normalize:\n",
    "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Process the inputs to produce the node_embeddings.\n",
    "\n",
    "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
    "            -node_representations: tensor with shape (num_nodes,feature_dim)\n",
    "            -edges: tensor with shape (num_edges,2) \n",
    "            -edge_weights:with shape (num_edges,), as in our problem all the edges\n",
    "            have the same weight this tensor is going to be a ones array\n",
    "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
    "        \"\"\"\n",
    "        node_representations, edges, edge_weights = inputs\n",
    "        # Divide the source and target indices\n",
    "        source_indexes = edges[:, 0]\n",
    "        target_indexes = edges[:, 1]\n",
    "        # Obtain the neighbour (target) representations\n",
    "        neighbour_representations = tf.gather(node_representations, target_indexes)\n",
    "        neighbour_messages = self.prepare(neighbour_representations, edge_weights)\n",
    "        aggregated_messages = self.aggregate(source_indexes, neighbour_messages, node_representations)\n",
    "        return self.update(node_representations, aggregated_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node Classifier model \n",
    "\n",
    "class GNNNodeClassifier(tf.keras.Model):\n",
    "    def __init__(self, num_classes, hidden_units, aggregation_type=\"mean\",\n",
    "                 combination_type=\"concat\", dropout_rate=0.2, normalize=True, *args, **kwargs):\n",
    "        super(GNNNodeClassifier, self).__init__(*args, **kwargs)\n",
    "        # Preprocessing: transform the node features\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Convolutional layers\n",
    "        self.conv1 = GraphConvLayer(hidden_units, dropout_rate, aggregation_type,\n",
    "                                    combination_type, normalize, name=\"graph_conv1\")\n",
    "        self.conv2 = GraphConvLayer(hidden_units, dropout_rate, aggregation_type,\n",
    "                                    combination_type, normalize, name=\"graph_conv2\")\n",
    "        # Postprocessing\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Final layer that produces the logits for each node\n",
    "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Inputs should be a tuple of:\n",
    "        (node_features, edges, edge_weights, input_node_indices)\n",
    "        where:\n",
    "            - node_features: tensor with shape (batch_size, num_nodes, feature_dim)\n",
    "            - edges: tensor with shape (batch_size, num_edges, 2)\n",
    "            - edge_weights: tensor with shape (batch_size, num_edges)\n",
    "            - node_indices: tensor with shape (batch_size, num_nodes)\n",
    "        Each input corresponds to a graph/image\n",
    "\n",
    "        \"\"\"\n",
    "        node_features, edges, edge_weights, node_indices = inputs\n",
    "\n",
    "        # Function that processes a single graph\n",
    "        def process_graph(single_inputs):\n",
    "            nf, e, ew, ni = single_inputs  # nf: (num_nodes, feature_dim), e: (num_edges, 2), etc.\n",
    "            x = self.preprocess(nf)  # x: (num_nodes, hidden_dim)\n",
    "            x1 = self.conv1((x, e, ew))\n",
    "            x = x + x1  \n",
    "            x2 = self.conv2((x, e, ew))\n",
    "            x = x + x2  \n",
    "            x = self.postprocess(x)\n",
    "            # Obtain the representations for each node\n",
    "            node_emb = tf.gather(x, ni)\n",
    "            logits = self.compute_logits(node_emb)  # (num_nodes, num_classes)\n",
    "            return logits\n",
    "\n",
    "        # Apply tf.map_fn for processing each graph of the batch\n",
    "        outputs = tf.map_fn(process_graph, (node_features, edges, edge_weights, node_indices),\n",
    "                            fn_output_signature=tf.float32)\n",
    "        # outputs have shape:(batch_size, num_nodes, num_classes)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for extracting data from the dataframe and building the dataset\n",
    "def extract_graph_data(df, image_id):\n",
    "    \"\"\"\n",
    "    Extracts data of the graph for a given image\n",
    "      - Filters the rows with image_id.\n",
    "      - Uses columns 'x' and 'y' por Voronoi tessellation\n",
    "      - Extracts features for each node \n",
    "      - Label is the column 'activity'\n",
    "    \"\"\"\n",
    "    df_img = df[df['image_id'] == image_id].reset_index(drop=True)\n",
    "    num_nodes = df_img.shape[0]\n",
    "    \n",
    "    points = df_img[['x', 'y']].to_numpy()\n",
    "    vor = Voronoi(points)\n",
    "    # Obtains the edges (a pair of points) for the Voronoi tessellation\n",
    "    if len(vor.ridge_points) > 0:\n",
    "        edges = np.array(vor.ridge_points, dtype=np.int32)\n",
    "    else:\n",
    "        edges = np.empty((0, 2), dtype=np.int32)\n",
    "    num_edges = edges.shape[0]\n",
    "   \n",
    "    edge_weights = np.ones((num_edges,), dtype=np.float32)\n",
    "       \n",
    "    feature_cols = [col for col in df_img.columns if col not in ['image_id', 'x', 'y', 'activity','label','type']]\n",
    "    node_features = df_img[feature_cols].to_numpy().astype(np.float32)\n",
    "\n",
    "    labels = df_img['activity'].to_numpy().astype(np.int32)\n",
    "    # Modes indexes: just from 0 to num_nodes-1\n",
    "    node_indexes = np.arange(num_nodes, dtype=np.int32)\n",
    "    \n",
    "    return node_features, edges, edge_weights, node_indexes, labels\n",
    "\n",
    "#Creates a tf.data.Dataset from a dataframe\n",
    "def create_graph_dataset(df, batch_size, feature_dim):\n",
    "    image_ids = df['image_id'].unique()\n",
    "    \n",
    "    def gen():\n",
    "        for img_id in image_ids:\n",
    "            node_features, edges, edge_weights, node_indices, labels = extract_graph_data(df, img_id)\n",
    "            # Ensure that the shapes are correct:\n",
    "            node_features = np.reshape(node_features, (-1, feature_dim))\n",
    "            # edges with shape:(num_edges, 2)\n",
    "            edges = np.reshape(edges, (-1, 2))\n",
    "            edge_weights = np.reshape(edge_weights, (-1,))\n",
    "            node_indices = np.reshape(node_indices, (-1,))\n",
    "            labels = np.reshape(labels, (-1,))\n",
    "            yield (node_features, edges, edge_weights, node_indices), labels\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=(None, feature_dim), dtype=tf.float32),  # node_features\n",
    "                tf.TensorSpec(shape=(None, 2), dtype=tf.int32),              # edges\n",
    "                tf.TensorSpec(shape=(None,), dtype=tf.float32),              # edge_weights\n",
    "                tf.TensorSpec(shape=(None,), dtype=tf.int32),                # node_indices\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(None,), dtype=tf.int32)  # labels\n",
    "        )\n",
    "    )\n",
    "    # Use padded_batch para handling graphs with a varying number of edges\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=(\n",
    "            (\n",
    "                tf.TensorShape([None, feature_dim]),  # node_features\n",
    "                tf.TensorShape([None, 2]),              # edges\n",
    "                tf.TensorShape([None]),                # edge_weights\n",
    "                tf.TensorShape([None]),                # node_indices\n",
    "            ),\n",
    "            tf.TensorShape([None])  # labels\n",
    "        ),\n",
    "        padding_values=(\n",
    "            (\n",
    "                tf.constant(0, dtype=tf.float32),\n",
    "                tf.constant(0, dtype=tf.int32),\n",
    "                tf.constant(0, dtype=tf.float32),\n",
    "                tf.constant(0, dtype=tf.int32),\n",
    "            ),\n",
    "            tf.constant(-1, dtype=tf.int32)\n",
    "        )\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the input data\n",
    "density=0.2\n",
    "fa=100\n",
    "input_file=f'phia{density}/traj_phia{density}-T05-Fa{fa}-tau1.dat'\n",
    "df=pd.read_csv(input_file, sep='\\s+',names=[\"label\", \"type\", \"x\", \"y\"])\n",
    "cols_names=['area', 'perimeter', 'neighbours', 'max neighbour distance',\n",
    "       'min neighbour distance', 'max vertices distance',\n",
    "       'min vertices distance', 'max vertices-point distance',\n",
    "       'min vertices-point distance', 'distance to center', 'activity',\n",
    "       'particle type']\n",
    "input_file2=f\"phia{density}/particles-features-{density}-Fa{fa}.txt\"\n",
    "df2=pd.read_csv(input_file2, sep='\\s+',names=cols_names)\n",
    "\n",
    "#Create a dataframe that includes both, the voronoi features and the particle positions\n",
    "df=df[0:2_000_000].join(df2)\n",
    "df['image_id']=np.floor(df.index/1000) #Add a column with the id of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score,roc_auc_score\n",
    "feature_cols = [col for col in df.columns if col not in ['image_id', 'x', 'y', 'activity','label','type']]\n",
    "feature_dim = len(feature_cols)\n",
    "\n",
    "# Model parameters\n",
    "num_classes = 2       \n",
    "hidden_units = [64, 64]\n",
    "dropout_rate = 0.2\n",
    "aggregation_type = \"mean\"\n",
    "combination_type = \"concat\"\n",
    "normalize = True\n",
    "batch_size = 1  \n",
    "\n",
    "images_ids=df['image_id'].unique()\n",
    "train_images_ids,test_images_ids=train_test_split(images_ids,random_state=50,test_size=0.2)\n",
    "train_df=df[df['image_id'].isin(train_images_ids)].reset_index(drop=True)\n",
    "test_df=df[df['image_id'].isin(test_images_ids)].reset_index(drop=True)\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = create_graph_dataset(train_df, batch_size, feature_dim)\n",
    "test_dataset = create_graph_dataset(test_df, batch_size, feature_dim)\n",
    "\n",
    "# Instance and compile the GNN model\n",
    "model = GNNNodeClassifier(num_classes, hidden_units, aggregation_type, combination_type, dropout_rate, normalize)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.7887 - loss: 0.5027\n",
      "Epoch 2/10\n",
      "\u001b[1m  8/320\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.8055 - loss: 0.4654"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.8095 - loss: 0.4590\n",
      "Epoch 3/10\n",
      "\u001b[1m  9/320\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8116 - loss: 0.4464"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.8142 - loss: 0.4474\n",
      "Epoch 4/10\n",
      "\u001b[1m  9/320\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8156 - loss: 0.4419"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.8167 - loss: 0.4419\n",
      "Epoch 5/10\n",
      "\u001b[1m  9/320\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8179 - loss: 0.4382"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.8180 - loss: 0.4384\n",
      "Epoch 6/10\n",
      "\u001b[1m  9/320\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.8174 - loss: 0.4358"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.8187 - loss: 0.4355\n",
      "Epoch 7/10\n",
      "\u001b[1m  7/320\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.8173 - loss: 0.4315"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.8194 - loss: 0.4311\n",
      "Epoch 8/10\n",
      "\u001b[1m  8/320\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8179 - loss: 0.4282"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.8207 - loss: 0.4273\n",
      "Epoch 9/10\n",
      "\u001b[1m  7/320\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8211 - loss: 0.4232"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.8215 - loss: 0.4246\n",
      "Epoch 10/10\n",
      "\u001b[1m  8/320\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8228 - loss: 0.4210"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.8223 - loss: 0.4231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2126e24bdf0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8242 - loss: 0.4178\n",
      "0.8239752650260925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "accuracy=model.evaluate(test_dataset)\n",
    "print(accuracy[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for (node_features, edges, edge_weights, node_indices), labels in test_dataset:\n",
    "    predictions = model((node_features, edges, edge_weights, node_indices))\n",
    "    # probabilities = tf.nn.sigmoid(predictions).numpy()  # For binary classification\n",
    "    probabilities = keras.activations.softmax(tf.convert_to_tensor(predictions)).numpy()\n",
    "    decision=np.floor(probabilities[:,:,1]*2)\n",
    "    all_labels.extend(labels.numpy().flatten())\n",
    "    all_predictions.extend(decision.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.5899382716049383\n",
      "Test F1: 0.3123372238834764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90     64800\n",
      "           1       0.71      0.20      0.31     16200\n",
      "\n",
      "    accuracy                           0.82     81000\n",
      "   macro avg       0.77      0.59      0.61     81000\n",
      "weighted avg       0.81      0.82      0.78     81000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,classification_report\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "f1= f1_score(all_labels,all_predictions)\n",
    "print(\"Test AUC:\", auc)\n",
    "print('Test F1:',f1)\n",
    "print(classification_report(all_labels,all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7159844,\n",
       " 0.2645995,\n",
       " 0.7345969,\n",
       " 0.28207645,\n",
       " 0.6380576,\n",
       " 0.3489702,\n",
       " 0.7453385,\n",
       " 0.29966557,\n",
       " 0.33867678,\n",
       " 0.6462641,\n",
       " 0.46524718,\n",
       " 0.48047706,\n",
       " 0.77662885,\n",
       " 0.27569345,\n",
       " 0.6975681,\n",
       " 0.26362994,\n",
       " 0.70750976,\n",
       " 0.2806128,\n",
       " 0.7573876,\n",
       " 0.2490431,\n",
       " 0.6658109,\n",
       " 0.32538983,\n",
       " 0.35746148,\n",
       " 0.6140405,\n",
       " 0.747182,\n",
       " 0.24801345,\n",
       " 0.6074776,\n",
       " 0.35634506,\n",
       " 0.7883099,\n",
       " 0.20904529,\n",
       " 0.6121916,\n",
       " 0.34460166,\n",
       " 0.7900176,\n",
       " 0.23764397,\n",
       " 0.31713736,\n",
       " 0.62390727,\n",
       " 0.79013336,\n",
       " 0.24650258,\n",
       " 0.4382181,\n",
       " 0.5394414,\n",
       " 0.6088959,\n",
       " 0.36731318,\n",
       " 0.5242623,\n",
       " 0.4402898,\n",
       " 0.8564378,\n",
       " 0.17809601,\n",
       " 0.42063662,\n",
       " 0.56673616,\n",
       " 0.49703878,\n",
       " 0.46606138,\n",
       " 0.75259066,\n",
       " 0.31260177,\n",
       " 0.6609633,\n",
       " 0.31887963,\n",
       " 0.7741204,\n",
       " 0.27057046,\n",
       " 0.66150296,\n",
       " 0.34389865,\n",
       " 0.87218475,\n",
       " 0.14609902,\n",
       " 0.6541497,\n",
       " 0.33856723,\n",
       " 0.5462443,\n",
       " 0.42175418,\n",
       " 0.521305,\n",
       " 0.45328853,\n",
       " 0.800982,\n",
       " 0.25545135,\n",
       " 0.7255122,\n",
       " 0.26031727,\n",
       " 0.7967296,\n",
       " 0.31771147,\n",
       " 0.6508306,\n",
       " 0.3487359,\n",
       " 0.6544872,\n",
       " 0.31844813,\n",
       " 0.17745602,\n",
       " 0.75326276,\n",
       " 0.8638974,\n",
       " 0.19800606,\n",
       " 0.49569923,\n",
       " 0.47653395,\n",
       " 0.2216364,\n",
       " 0.6833663,\n",
       " 0.40693504,\n",
       " 0.56503,\n",
       " 0.34696302,\n",
       " 0.5971468,\n",
       " 0.5899101,\n",
       " 0.37444827,\n",
       " 0.7023085,\n",
       " 0.2926166,\n",
       " 0.72341174,\n",
       " 0.30253613,\n",
       " 0.78517014,\n",
       " 0.25843167,\n",
       " 0.28434095,\n",
       " 0.6437926,\n",
       " 0.7886097,\n",
       " 0.2255725,\n",
       " 0.7980218,\n",
       " 0.28736144,\n",
       " 0.6652628,\n",
       " 0.36310494,\n",
       " 0.74072963,\n",
       " 0.27471477,\n",
       " 0.8496825,\n",
       " 0.23643544,\n",
       " 0.65181935,\n",
       " 0.38380837,\n",
       " 0.6505548,\n",
       " 0.33426762,\n",
       " 0.5964989,\n",
       " 0.38471356,\n",
       " 0.48783374,\n",
       " 0.46651345,\n",
       " 0.83970696,\n",
       " 0.13025902,\n",
       " 0.8023373,\n",
       " 0.30579463,\n",
       " 0.701017,\n",
       " 0.28814512,\n",
       " 0.8009526,\n",
       " 0.27340162,\n",
       " 0.74523836,\n",
       " 0.27432734,\n",
       " 0.7570613,\n",
       " 0.2948649,\n",
       " 0.22520147,\n",
       " 0.67590976,\n",
       " 0.68808997,\n",
       " 0.28513968,\n",
       " 0.7242957,\n",
       " 0.28260195,\n",
       " 0.56638837,\n",
       " 0.40610132,\n",
       " 0.31125602,\n",
       " 0.63570577,\n",
       " 0.4133289,\n",
       " 0.54538745,\n",
       " 0.20155703,\n",
       " 0.73725754,\n",
       " 0.6537204,\n",
       " 0.3234712,\n",
       " 0.5148212,\n",
       " 0.4402457,\n",
       " 0.7472399,\n",
       " 0.23840384,\n",
       " 0.38270706,\n",
       " 0.5854888,\n",
       " 0.37846836,\n",
       " 0.5919676,\n",
       " 0.61052877,\n",
       " 0.3494455,\n",
       " 0.64831406,\n",
       " 0.33081138,\n",
       " 0.8160329,\n",
       " 0.18920696,\n",
       " 0.18392113,\n",
       " 0.7658913,\n",
       " 0.9156691,\n",
       " 0.09981875,\n",
       " 0.156871,\n",
       " 0.79048514,\n",
       " 0.5545936,\n",
       " 0.40174702,\n",
       " 0.35970247,\n",
       " 0.5734496,\n",
       " 0.8128653,\n",
       " 0.18259482,\n",
       " 0.7456731,\n",
       " 0.24265005,\n",
       " 0.6396773,\n",
       " 0.34963363,\n",
       " 0.49050385,\n",
       " 0.45429552,\n",
       " 0.6668996,\n",
       " 0.28458905,\n",
       " 0.7011998,\n",
       " 0.26038688,\n",
       " 0.44453695,\n",
       " 0.4922936,\n",
       " 0.39427075,\n",
       " 0.55919987,\n",
       " 0.7203973,\n",
       " 0.27530825,\n",
       " 0.48872668,\n",
       " 0.47690535,\n",
       " 0.37150866,\n",
       " 0.5921675,\n",
       " 0.882553,\n",
       " 0.10054979,\n",
       " 0.58508575,\n",
       " 0.4197997,\n",
       " 0.77312934,\n",
       " 0.22047928,\n",
       " 0.83734596,\n",
       " 0.17170352,\n",
       " 0.6263022,\n",
       " 0.34997836,\n",
       " 0.65496707,\n",
       " 0.36282793,\n",
       " 0.8434804,\n",
       " 0.15506274,\n",
       " 0.8249762,\n",
       " 0.22150789,\n",
       " 0.73607093,\n",
       " 0.22410335,\n",
       " 0.8295124,\n",
       " 0.19130126,\n",
       " 0.9057121,\n",
       " 0.068292215,\n",
       " 0.84787315,\n",
       " 0.24066955,\n",
       " 0.7304851,\n",
       " 0.29151556,\n",
       " 0.79848504,\n",
       " 0.22227144,\n",
       " 0.63897055,\n",
       " 0.31611967,\n",
       " 0.7941302,\n",
       " 0.20281737,\n",
       " 0.8237251,\n",
       " 0.28334907,\n",
       " 0.7656792,\n",
       " 0.28151816,\n",
       " 0.75431,\n",
       " 0.2453403,\n",
       " 0.78798187,\n",
       " 0.18813387,\n",
       " 0.8291934,\n",
       " 0.21526878,\n",
       " 0.794792,\n",
       " 0.2153187,\n",
       " 0.57489586,\n",
       " 0.38784996,\n",
       " 0.7548623,\n",
       " 0.29483357,\n",
       " 0.5877975,\n",
       " 0.3901963,\n",
       " 0.7197009,\n",
       " 0.2905031,\n",
       " 0.5124079,\n",
       " 0.45941845,\n",
       " 0.8341334,\n",
       " 0.17638755,\n",
       " 0.80420274,\n",
       " 0.1901687,\n",
       " 0.89309645,\n",
       " 0.080476396,\n",
       " 0.8681049,\n",
       " 0.17300059,\n",
       " 0.79668427,\n",
       " 0.27720863,\n",
       " 0.7655717,\n",
       " 0.19499338,\n",
       " 0.8071726,\n",
       " 0.18634544,\n",
       " 0.62999594,\n",
       " 0.3794632,\n",
       " 0.71489763,\n",
       " 0.32735604,\n",
       " 0.7546136,\n",
       " 0.23832154,\n",
       " 0.6888946,\n",
       " 0.35001433,\n",
       " 0.8553336,\n",
       " 0.13259692,\n",
       " 0.76982415,\n",
       " 0.24699053,\n",
       " 0.72281027,\n",
       " 0.31222934,\n",
       " 0.7837998,\n",
       " 0.27852505,\n",
       " 0.7683172,\n",
       " 0.25985074,\n",
       " 0.7714239,\n",
       " 0.21950823,\n",
       " 0.85329664,\n",
       " 0.21650942,\n",
       " 0.83793265,\n",
       " 0.14291093,\n",
       " 0.80959016,\n",
       " 0.15743484,\n",
       " 0.858201,\n",
       " 0.13583013,\n",
       " 0.7584094,\n",
       " 0.249688,\n",
       " 0.8640521,\n",
       " 0.11530446,\n",
       " 0.7272416,\n",
       " 0.26859757,\n",
       " 0.71488,\n",
       " 0.2523731,\n",
       " 0.82494926,\n",
       " 0.23220877,\n",
       " 0.78256726,\n",
       " 0.23330659,\n",
       " 0.8044026,\n",
       " 0.22178201,\n",
       " 0.50588006,\n",
       " 0.4878219,\n",
       " 0.79515153,\n",
       " 0.15978254,\n",
       " 0.7000268,\n",
       " 0.26748586,\n",
       " 0.733712,\n",
       " 0.24287829,\n",
       " 0.6046902,\n",
       " 0.34784406,\n",
       " 0.58236635,\n",
       " 0.37686107,\n",
       " 0.8530978,\n",
       " 0.13125916,\n",
       " 0.7672643,\n",
       " 0.26133922,\n",
       " 0.77737474,\n",
       " 0.20452759,\n",
       " 0.5136354,\n",
       " 0.44277635,\n",
       " 0.8394762,\n",
       " 0.15899603,\n",
       " 0.7349448,\n",
       " 0.22653937,\n",
       " 0.7958701,\n",
       " 0.31155872,\n",
       " 0.8941876,\n",
       " 0.072103605,\n",
       " 0.81104004,\n",
       " 0.2865422,\n",
       " 0.62696993,\n",
       " 0.3665262,\n",
       " 0.8738999,\n",
       " 0.119429365,\n",
       " 0.7360186,\n",
       " 0.29200962,\n",
       " 0.6827292,\n",
       " 0.3073363,\n",
       " 0.78604525,\n",
       " 0.22444993,\n",
       " 0.79497457,\n",
       " 0.18166421,\n",
       " 0.74306434,\n",
       " 0.24462922,\n",
       " 0.71061236,\n",
       " 0.28101572,\n",
       " 0.80449635,\n",
       " 0.26255643,\n",
       " 0.82522565,\n",
       " 0.2283643,\n",
       " 0.6388059,\n",
       " 0.32048026,\n",
       " 0.5907084,\n",
       " 0.4283619,\n",
       " 0.9120518,\n",
       " 0.24550655,\n",
       " 0.8553503,\n",
       " 0.156287,\n",
       " 0.7416616,\n",
       " 0.2763684,\n",
       " 0.7313901,\n",
       " 0.28328136,\n",
       " 0.8938898,\n",
       " 0.07537002,\n",
       " 0.8867051,\n",
       " 0.21297273,\n",
       " 0.6234399,\n",
       " 0.34492204,\n",
       " 0.41881335,\n",
       " 0.534745,\n",
       " 0.79259497,\n",
       " 0.26309144,\n",
       " 0.7960601,\n",
       " 0.24535054,\n",
       " 0.69553506,\n",
       " 0.31071553,\n",
       " 0.9005712,\n",
       " 0.07061205,\n",
       " 0.86465055,\n",
       " 0.27397364,\n",
       " 0.3967925,\n",
       " 0.6199253,\n",
       " 0.835291,\n",
       " 0.32728192,\n",
       " 0.7757295,\n",
       " 0.23630084,\n",
       " 0.56304294,\n",
       " 0.4258144,\n",
       " 0.7645945,\n",
       " 0.22749093,\n",
       " 0.43909106,\n",
       " 0.52109224,\n",
       " 0.7739382,\n",
       " 0.23523587,\n",
       " 0.8115224,\n",
       " 0.17509441,\n",
       " 0.8820899,\n",
       " 0.13242799,\n",
       " 0.82699823,\n",
       " 0.20089903,\n",
       " 0.76487935,\n",
       " 0.2622218,\n",
       " 0.7764705,\n",
       " 0.26533446,\n",
       " 0.9063904,\n",
       " 0.1956671,\n",
       " 0.7648416,\n",
       " 0.2701231,\n",
       " 0.84056294,\n",
       " 0.12805928,\n",
       " 0.523442,\n",
       " 0.46488664,\n",
       " 0.7473338,\n",
       " 0.24897924,\n",
       " 0.82549435,\n",
       " 0.19713683,\n",
       " 0.7533464,\n",
       " 0.256057,\n",
       " 0.6539949,\n",
       " 0.3943819,\n",
       " 0.73986167,\n",
       " 0.26924944,\n",
       " 0.78157663,\n",
       " 0.21480556,\n",
       " 0.81090844,\n",
       " 0.15001278,\n",
       " 0.76992536,\n",
       " 0.27232975,\n",
       " 0.7783693,\n",
       " 0.2019039,\n",
       " 0.80327386,\n",
       " 0.2133503,\n",
       " 0.71919733,\n",
       " 0.34062538,\n",
       " 0.8222772,\n",
       " 0.16905059,\n",
       " 0.8554671,\n",
       " 0.11674455,\n",
       " 0.7492426,\n",
       " 0.25906956,\n",
       " 0.7659658,\n",
       " 0.2219551,\n",
       " 0.78299534,\n",
       " 0.27472964,\n",
       " 0.87912625,\n",
       " 0.113078564,\n",
       " 0.89085174,\n",
       " 0.22339003,\n",
       " 0.78241014,\n",
       " 0.24455564,\n",
       " 0.64263034,\n",
       " 0.33564362,\n",
       " 0.7887409,\n",
       " 0.20084965,\n",
       " 0.837816,\n",
       " 0.16689058,\n",
       " 0.80885196,\n",
       " 0.27158844,\n",
       " 0.73985004,\n",
       " 0.3080005,\n",
       " 0.85942644,\n",
       " 0.1671623,\n",
       " 0.8194887,\n",
       " 0.29131362,\n",
       " 0.8463153,\n",
       " 0.14807376,\n",
       " 0.7623545,\n",
       " 0.22190447,\n",
       " 0.73373777,\n",
       " 0.3029009,\n",
       " 0.86579216,\n",
       " 0.105950706,\n",
       " 0.73657715,\n",
       " 0.29515776,\n",
       " 0.7180579,\n",
       " 0.23963897,\n",
       " 0.84041697,\n",
       " 0.23671639,\n",
       " 0.8286314,\n",
       " 0.21927345,\n",
       " 0.7405937,\n",
       " 0.24283482,\n",
       " 0.8424852,\n",
       " 0.24128217,\n",
       " 0.8070015,\n",
       " 0.21343195,\n",
       " 0.815614,\n",
       " 0.19946532,\n",
       " 0.73853856,\n",
       " 0.27097267,\n",
       " 0.83783597,\n",
       " 0.24017999,\n",
       " 0.77927655,\n",
       " 0.27981812,\n",
       " 0.7305288,\n",
       " 0.24444433,\n",
       " 0.7769372,\n",
       " 0.33619574,\n",
       " 0.6563402,\n",
       " 0.35493422,\n",
       " 0.78456014,\n",
       " 0.24280614,\n",
       " 0.7484742,\n",
       " 0.2593418,\n",
       " 0.7006305,\n",
       " 0.32722557,\n",
       " 0.72556674,\n",
       " 0.2505721,\n",
       " 0.8262377,\n",
       " 0.1339198,\n",
       " 0.8052333,\n",
       " 0.21960343,\n",
       " 0.8529236,\n",
       " 0.14962076,\n",
       " 0.839781,\n",
       " 0.12877382,\n",
       " 0.8371759,\n",
       " 0.15817055,\n",
       " 0.6743075,\n",
       " 0.3041727,\n",
       " 0.80486566,\n",
       " 0.33116573,\n",
       " 0.34329852,\n",
       " 0.5862207,\n",
       " 0.79757,\n",
       " 0.221619,\n",
       " 0.82136947,\n",
       " 0.15201528,\n",
       " 0.8465244,\n",
       " 0.18084668,\n",
       " 0.74679005,\n",
       " 0.24651755,\n",
       " 0.7857635,\n",
       " 0.24998015,\n",
       " 0.8163578,\n",
       " 0.18263957,\n",
       " 0.8634078,\n",
       " 0.18197562,\n",
       " 0.78532493,\n",
       " 0.1922944,\n",
       " 0.73266727,\n",
       " 0.28188172,\n",
       " 0.7071344,\n",
       " 0.30399933,\n",
       " 0.8730145,\n",
       " 0.11160576,\n",
       " 0.70629334,\n",
       " 0.32186893,\n",
       " 0.83136374,\n",
       " 0.17997977,\n",
       " 0.897695,\n",
       " 0.09562068,\n",
       " 0.8005788,\n",
       " 0.2451253,\n",
       " 0.76065195,\n",
       " 0.25192213,\n",
       " 0.87901,\n",
       " 0.15761411,\n",
       " 0.71824294,\n",
       " 0.25601587,\n",
       " 0.68034637,\n",
       " 0.32541248,\n",
       " 0.83554196,\n",
       " 0.2446876,\n",
       " 0.80260456,\n",
       " 0.23866485,\n",
       " 0.80013126,\n",
       " 0.19169848,\n",
       " 0.73072666,\n",
       " 0.27844605,\n",
       " 0.8331048,\n",
       " 0.2153949,\n",
       " 0.76930374,\n",
       " 0.27504647,\n",
       " 0.7765574,\n",
       " 0.21411563,\n",
       " 0.8409072,\n",
       " 0.1717616,\n",
       " 0.75052357,\n",
       " 0.26024848,\n",
       " 0.8024863,\n",
       " 0.19686553,\n",
       " 0.7989922,\n",
       " 0.20905857,\n",
       " 0.84410226,\n",
       " 0.22098756,\n",
       " 0.7284158,\n",
       " 0.24906382,\n",
       " 0.6657487,\n",
       " 0.3015067,\n",
       " 0.696923,\n",
       " 0.30744419,\n",
       " 0.7647095,\n",
       " 0.31046,\n",
       " 0.767295,\n",
       " 0.25774935,\n",
       " 0.8489821,\n",
       " 0.13686262,\n",
       " 0.83576655,\n",
       " 0.14319333,\n",
       " 0.71414566,\n",
       " 0.27717286,\n",
       " 0.7393721,\n",
       " 0.2536136,\n",
       " 0.8493974,\n",
       " 0.1682733,\n",
       " 0.81708944,\n",
       " 0.20553055,\n",
       " 0.73618084,\n",
       " 0.25403833,\n",
       " 0.87736756,\n",
       " 0.14351071,\n",
       " 0.72196317,\n",
       " 0.30684364,\n",
       " 0.8350185,\n",
       " 0.2597102,\n",
       " 0.76742536,\n",
       " 0.22998989,\n",
       " 0.69175047,\n",
       " 0.33840916,\n",
       " 0.66286045,\n",
       " 0.3050339,\n",
       " 0.8541837,\n",
       " 0.14271402,\n",
       " 0.78875124,\n",
       " 0.1929814,\n",
       " 0.8499332,\n",
       " 0.25959715,\n",
       " 0.7762857,\n",
       " 0.24303171,\n",
       " 0.77197075,\n",
       " 0.25239995,\n",
       " 0.81562114,\n",
       " 0.251687,\n",
       " 0.6971695,\n",
       " 0.29203758,\n",
       " 0.8742609,\n",
       " 0.14213268,\n",
       " 0.6906428,\n",
       " 0.3374228,\n",
       " 0.67893785,\n",
       " 0.34134325,\n",
       " 0.7662243,\n",
       " 0.21352796,\n",
       " 0.82185584,\n",
       " 0.15158144,\n",
       " 0.8055213,\n",
       " 0.24643615,\n",
       " 0.90711087,\n",
       " 0.24068962,\n",
       " 0.7462,\n",
       " 0.24592458,\n",
       " 0.78949445,\n",
       " 0.24539416,\n",
       " 0.8485314,\n",
       " 0.14277457,\n",
       " 0.72757906,\n",
       " 0.2569241,\n",
       " 0.763607,\n",
       " 0.27854967,\n",
       " 0.7183193,\n",
       " 0.26282352,\n",
       " 0.8516861,\n",
       " 0.15426011,\n",
       " 0.8071412,\n",
       " 0.16449738,\n",
       " 0.8507348,\n",
       " 0.19578294,\n",
       " 0.8560282,\n",
       " 0.13632102,\n",
       " 0.85504407,\n",
       " 0.13502412,\n",
       " 0.8122714,\n",
       " 0.20762558,\n",
       " 0.75404584,\n",
       " 0.2419015,\n",
       " 0.84080863,\n",
       " 0.16186632,\n",
       " 0.6410393,\n",
       " 0.36430505,\n",
       " 0.6484367,\n",
       " 0.3200487,\n",
       " 0.78230375,\n",
       " 0.23663625,\n",
       " 0.8234903,\n",
       " 0.24906762,\n",
       " 0.8256087,\n",
       " 0.1420901,\n",
       " 0.79564553,\n",
       " 0.28785822,\n",
       " 0.7538142,\n",
       " 0.2350799,\n",
       " 0.70283973,\n",
       " 0.2465217,\n",
       " 0.79295874,\n",
       " 0.25207463,\n",
       " 0.79446703,\n",
       " 0.20529576,\n",
       " 0.86631227,\n",
       " 0.2167812,\n",
       " 0.82550216,\n",
       " 0.19373012,\n",
       " 0.74323237,\n",
       " 0.24135211,\n",
       " 0.71927565,\n",
       " 0.27098697,\n",
       " 0.8679715,\n",
       " 0.13055746,\n",
       " 0.7927413,\n",
       " 0.22694497,\n",
       " 0.85056,\n",
       " 0.15648656,\n",
       " 0.80743414,\n",
       " 0.21683425,\n",
       " 0.836653,\n",
       " 0.19651271,\n",
       " 0.74094886,\n",
       " 0.2719418,\n",
       " 0.7748873,\n",
       " 0.2367825,\n",
       " 0.7950682,\n",
       " 0.19327132,\n",
       " 0.82949764,\n",
       " 0.15501292,\n",
       " 0.7801254,\n",
       " 0.24663515,\n",
       " 0.8421228,\n",
       " 0.23116714,\n",
       " 0.8026407,\n",
       " 0.21161383,\n",
       " 0.81849635,\n",
       " 0.31468675,\n",
       " 0.74926144,\n",
       " 0.2563492,\n",
       " 0.73010164,\n",
       " 0.25274542,\n",
       " 0.8714165,\n",
       " 0.24498361,\n",
       " 0.8454324,\n",
       " 0.19908352,\n",
       " 0.84111464,\n",
       " 0.19357201,\n",
       " 0.4751947,\n",
       " 0.47150663,\n",
       " 0.8335234,\n",
       " 0.18894024,\n",
       " 0.8085039,\n",
       " 0.21958587,\n",
       " 0.68029875,\n",
       " 0.32335666,\n",
       " 0.7035243,\n",
       " 0.29729882,\n",
       " 0.80762434,\n",
       " 0.18018125,\n",
       " 0.76343197,\n",
       " 0.21241347,\n",
       " 0.7068675,\n",
       " 0.24628647,\n",
       " 0.839046,\n",
       " 0.1342268,\n",
       " 0.8341801,\n",
       " 0.18789157,\n",
       " 0.87501013,\n",
       " 0.2289699,\n",
       " 0.6404213,\n",
       " 0.3345791,\n",
       " 0.74471354,\n",
       " 0.32196105,\n",
       " 0.78620255,\n",
       " 0.23887649,\n",
       " 0.79006135,\n",
       " 0.24863401,\n",
       " 0.80840015,\n",
       " 0.23438098,\n",
       " 0.7861223,\n",
       " 0.27670395,\n",
       " 0.7576167,\n",
       " 0.22046646,\n",
       " 0.8726849,\n",
       " 0.25905144,\n",
       " 0.8356958,\n",
       " 0.15993658,\n",
       " 0.76324344,\n",
       " 0.25739968,\n",
       " 0.8330884,\n",
       " 0.17422058,\n",
       " 0.85355574,\n",
       " 0.16409598,\n",
       " 0.83609533,\n",
       " 0.17014198,\n",
       " 0.78091604,\n",
       " 0.21495208,\n",
       " 0.8572787,\n",
       " 0.22431427,\n",
       " 0.81183815,\n",
       " 0.27553922,\n",
       " 0.69192487,\n",
       " 0.29286936,\n",
       " 0.88980407,\n",
       " 0.13481063,\n",
       " 0.6770121,\n",
       " 0.3186094,\n",
       " 0.8424465,\n",
       " 0.14570028,\n",
       " 0.7243534,\n",
       " 0.32423678,\n",
       " 0.76364696,\n",
       " 0.25803947,\n",
       " 0.89854467,\n",
       " 0.101737864,\n",
       " 0.865927,\n",
       " 0.14617926,\n",
       " 0.6972644,\n",
       " 0.31657055,\n",
       " 0.73824304,\n",
       " 0.3275055,\n",
       " 0.75095785,\n",
       " 0.23904312,\n",
       " 0.87966824,\n",
       " 0.23795742,\n",
       " 0.8467601,\n",
       " 0.16010575,\n",
       " 0.8621928,\n",
       " 0.12323546,\n",
       " 0.7044115,\n",
       " 0.32142773,\n",
       " 0.7179119,\n",
       " 0.2419406,\n",
       " 0.86661613,\n",
       " 0.19889042,\n",
       " 0.8850073,\n",
       " 0.08099968,\n",
       " 0.7251259,\n",
       " 0.3275757,\n",
       " 0.79922575,\n",
       " 0.26616728,\n",
       " 0.8337777,\n",
       " 0.25820577,\n",
       " 0.85205984,\n",
       " 0.22971056,\n",
       " 0.78142226,\n",
       " 0.2067206,\n",
       " 0.81513476,\n",
       " 0.18962301,\n",
       " 0.8101467,\n",
       " 0.23030847,\n",
       " 0.73087573,\n",
       " 0.26271746,\n",
       " 0.8858545,\n",
       " 0.08494341,\n",
       " 0.8530697,\n",
       " 0.27112204,\n",
       " 0.8213576,\n",
       " 0.15593974,\n",
       " 0.83517504,\n",
       " 0.17152865,\n",
       " 0.7307461,\n",
       " 0.35930604,\n",
       " 0.73729086,\n",
       " 0.26996613,\n",
       " 0.83749497,\n",
       " 0.15317358,\n",
       " 0.834846,\n",
       " 0.16940123,\n",
       " 0.6868556,\n",
       " 0.28634334,\n",
       " 0.8632901,\n",
       " 0.17584483,\n",
       " 0.7842985,\n",
       " 0.22339526,\n",
       " 0.81114286,\n",
       " 0.24901064,\n",
       " 0.810233,\n",
       " 0.24874789,\n",
       " 0.6533009,\n",
       " 0.36846447,\n",
       " 0.78490686,\n",
       " 0.19439971,\n",
       " 0.8197195,\n",
       " 0.16373858,\n",
       " 0.79758835,\n",
       " 0.24169032,\n",
       " 0.7758833,\n",
       " 0.2686987,\n",
       " 0.8652523,\n",
       " 0.13060337,\n",
       " 0.86450183,\n",
       " 0.19143593,\n",
       " 0.69558114,\n",
       " 0.2677983,\n",
       " 0.721331,\n",
       " 0.2920334,\n",
       " 0.8936289,\n",
       " 0.24876119,\n",
       " 0.7551458,\n",
       " 0.2318842,\n",
       " 0.7090659,\n",
       " 0.32196593,\n",
       " 0.49382,\n",
       " 0.53941137,\n",
       " 0.7497271,\n",
       " 0.27031326,\n",
       " 0.8738216,\n",
       " 0.22290549,\n",
       " 0.58807874,\n",
       " 0.40858513,\n",
       " 0.8774068,\n",
       " 0.17640468,\n",
       " 0.83764154,\n",
       " 0.2963058,\n",
       " 0.89130193,\n",
       " 0.08225567,\n",
       " 0.84044284,\n",
       " 0.18134569,\n",
       " 0.740145,\n",
       " 0.23013996,\n",
       " 0.836186,\n",
       " 0.23589423,\n",
       " 0.88058925,\n",
       " 0.12145172,\n",
       " 0.7956793,\n",
       " 0.2247097,\n",
       " 0.7771927,\n",
       " 0.2450242,\n",
       " 0.768603,\n",
       " 0.26841816,\n",
       " 0.7337981,\n",
       " 0.24409251,\n",
       " 0.86117035,\n",
       " 0.22420846,\n",
       " 0.8497239,\n",
       " 0.16877231,\n",
       " 0.912077,\n",
       " 0.06470919,\n",
       " 0.79699236,\n",
       " 0.19471437,\n",
       " 0.91992676,\n",
       " 0.09898748,\n",
       " 0.7614189,\n",
       " 0.32024992,\n",
       " 0.8189561,\n",
       " 0.17934893,\n",
       " 0.8482316,\n",
       " 0.13848129,\n",
       " 0.8340376,\n",
       " 0.18799162,\n",
       " 0.82360333,\n",
       " 0.18130444,\n",
       " 0.9050601,\n",
       " 0.115535066,\n",
       " 0.8603069,\n",
       " 0.19480439,\n",
       " 0.83709097,\n",
       " 0.17794234,\n",
       " 0.91662556,\n",
       " 0.059394807,\n",
       " 0.86606175,\n",
       " 0.18833157,\n",
       " 0.8590945,\n",
       " 0.23533398,\n",
       " 0.7459406,\n",
       " 0.24537705,\n",
       " 0.8545792,\n",
       " 0.17630956,\n",
       " 0.7984278,\n",
       " 0.28539476,\n",
       " 0.65187055,\n",
       " 0.33408743,\n",
       " 0.76622677,\n",
       " 0.24442625,\n",
       " 0.9007258,\n",
       " 0.18898937,\n",
       " 0.8616537,\n",
       " 0.1632039,\n",
       " 0.7068165,\n",
       " 0.28733546,\n",
       " 0.8231702,\n",
       " 0.19997694,\n",
       " 0.73580736,\n",
       " 0.25404975,\n",
       " 0.5596564,\n",
       " 0.40324178,\n",
       " 0.81024027,\n",
       " 0.21219698,\n",
       " 0.8578049,\n",
       " 0.17249307,\n",
       " 0.69964594,\n",
       " 0.26091814,\n",
       " 0.88044167,\n",
       " 0.13550459,\n",
       " 0.7752339,\n",
       " 0.32562944,\n",
       " 0.6970413,\n",
       " 0.27721825,\n",
       " 0.87473017,\n",
       " 0.22382386,\n",
       " 0.8845404,\n",
       " 0.20690984,\n",
       " 0.7966991,\n",
       " 0.20264983,\n",
       " ...]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score,roc_auc_score\n",
    "#Returns accuracy,auc,f1_score\n",
    "def score(y,pred):\n",
    "    return accuracy_score(y,pred),roc_auc_score(y,pred),f1_score(y,pred)\n",
    "pred_test=model.predict(test_dataset)\n",
    "# print(f'The accuracy on the test set is {accuracy_score(df_test_y,pred_test_gb)} and the AUC is {roc_auc_score(df_test_y,pred_test_gb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Arguments not recognized: {'metrics': ['roc_auc_score']}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroc_auc_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:393\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m use_cached_eval_dataset \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_use_cached_eval_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cached_eval_dataset:\n\u001b[0;32m    396\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments not recognized: {'metrics': ['roc_auc_score']}"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_dataset,metrics=['roc_auc_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>area</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>neighbours</th>\n",
       "      <th>max neighbour distance</th>\n",
       "      <th>min neighbour distance</th>\n",
       "      <th>max vertices distance</th>\n",
       "      <th>min vertices distance</th>\n",
       "      <th>max vertices-point distance</th>\n",
       "      <th>min vertices-point distance</th>\n",
       "      <th>distance to center</th>\n",
       "      <th>activity</th>\n",
       "      <th>particle type</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.71810</td>\n",
       "      <td>3.963970</td>\n",
       "      <td>0.992258</td>\n",
       "      <td>3.836169</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.756655</td>\n",
       "      <td>0.822074</td>\n",
       "      <td>1.436207</td>\n",
       "      <td>0.015668</td>\n",
       "      <td>0.884407</td>\n",
       "      <td>0.509452</td>\n",
       "      <td>0.129601</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19.83950</td>\n",
       "      <td>28.755500</td>\n",
       "      <td>1.222737</td>\n",
       "      <td>4.237809</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.405774</td>\n",
       "      <td>1.081656</td>\n",
       "      <td>1.501896</td>\n",
       "      <td>0.201827</td>\n",
       "      <td>0.794314</td>\n",
       "      <td>0.651656</td>\n",
       "      <td>0.021338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>14.11380</td>\n",
       "      <td>0.123613</td>\n",
       "      <td>1.021340</td>\n",
       "      <td>3.896370</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.549040</td>\n",
       "      <td>0.911955</td>\n",
       "      <td>1.440380</td>\n",
       "      <td>0.250894</td>\n",
       "      <td>0.854049</td>\n",
       "      <td>0.552541</td>\n",
       "      <td>0.084406</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>27.99440</td>\n",
       "      <td>27.836500</td>\n",
       "      <td>0.996716</td>\n",
       "      <td>3.825848</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.444316</td>\n",
       "      <td>0.855382</td>\n",
       "      <td>1.407457</td>\n",
       "      <td>0.115618</td>\n",
       "      <td>0.733849</td>\n",
       "      <td>0.540194</td>\n",
       "      <td>0.057512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20.58560</td>\n",
       "      <td>16.870600</td>\n",
       "      <td>0.861796</td>\n",
       "      <td>3.562893</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.212920</td>\n",
       "      <td>0.901367</td>\n",
       "      <td>1.226116</td>\n",
       "      <td>0.105015</td>\n",
       "      <td>0.700601</td>\n",
       "      <td>0.527187</td>\n",
       "      <td>0.036320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>996</td>\n",
       "      <td>4</td>\n",
       "      <td>17.00900</td>\n",
       "      <td>16.738400</td>\n",
       "      <td>1.398074</td>\n",
       "      <td>4.562085</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.706074</td>\n",
       "      <td>1.043705</td>\n",
       "      <td>1.564971</td>\n",
       "      <td>0.018557</td>\n",
       "      <td>0.901003</td>\n",
       "      <td>0.571301</td>\n",
       "      <td>0.128033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>997</td>\n",
       "      <td>4</td>\n",
       "      <td>27.12020</td>\n",
       "      <td>3.015330</td>\n",
       "      <td>0.644134</td>\n",
       "      <td>3.089516</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.205705</td>\n",
       "      <td>0.792358</td>\n",
       "      <td>1.066991</td>\n",
       "      <td>0.024285</td>\n",
       "      <td>0.604872</td>\n",
       "      <td>0.476485</td>\n",
       "      <td>0.031195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>998</td>\n",
       "      <td>4</td>\n",
       "      <td>28.79020</td>\n",
       "      <td>6.405500</td>\n",
       "      <td>0.777852</td>\n",
       "      <td>3.426046</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.183314</td>\n",
       "      <td>0.789633</td>\n",
       "      <td>1.177903</td>\n",
       "      <td>0.128303</td>\n",
       "      <td>0.631295</td>\n",
       "      <td>0.570359</td>\n",
       "      <td>0.036724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>999</td>\n",
       "      <td>4</td>\n",
       "      <td>11.37550</td>\n",
       "      <td>25.538700</td>\n",
       "      <td>0.648328</td>\n",
       "      <td>3.096628</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.859703</td>\n",
       "      <td>0.801224</td>\n",
       "      <td>1.098747</td>\n",
       "      <td>0.505184</td>\n",
       "      <td>0.608810</td>\n",
       "      <td>0.491049</td>\n",
       "      <td>0.020731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>9.69861</td>\n",
       "      <td>7.494090</td>\n",
       "      <td>0.721191</td>\n",
       "      <td>3.184995</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.036786</td>\n",
       "      <td>0.839039</td>\n",
       "      <td>1.120351</td>\n",
       "      <td>0.395579</td>\n",
       "      <td>0.567975</td>\n",
       "      <td>0.479669</td>\n",
       "      <td>0.018331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  type         x          y      area  perimeter  neighbours  \\\n",
       "0            1     1   8.71810   3.963970  0.992258   3.836169         8.0   \n",
       "1            2     1  19.83950  28.755500  1.222737   4.237809         6.0   \n",
       "2            3     1  14.11380   0.123613  1.021340   3.896370         6.0   \n",
       "3            4     1  27.99440  27.836500  0.996716   3.825848         6.0   \n",
       "4            5     1  20.58560  16.870600  0.861796   3.562893         6.0   \n",
       "...        ...   ...       ...        ...       ...        ...         ...   \n",
       "1599995    996     4  17.00900  16.738400  1.398074   4.562085         6.0   \n",
       "1599996    997     4  27.12020   3.015330  0.644134   3.089516         6.0   \n",
       "1599997    998     4  28.79020   6.405500  0.777852   3.426046         6.0   \n",
       "1599998    999     4  11.37550  25.538700  0.648328   3.096628         5.0   \n",
       "1599999   1000     4   9.69861   7.494090  0.721191   3.184995         6.0   \n",
       "\n",
       "         max neighbour distance  min neighbour distance  \\\n",
       "0                      1.756655                0.822074   \n",
       "1                      1.405774                1.081656   \n",
       "2                      1.549040                0.911955   \n",
       "3                      1.444316                0.855382   \n",
       "4                      1.212920                0.901367   \n",
       "...                         ...                     ...   \n",
       "1599995                1.706074                1.043705   \n",
       "1599996                1.205705                0.792358   \n",
       "1599997                1.183314                0.789633   \n",
       "1599998                0.859703                0.801224   \n",
       "1599999                1.036786                0.839039   \n",
       "\n",
       "         max vertices distance  min vertices distance  \\\n",
       "0                     1.436207               0.015668   \n",
       "1                     1.501896               0.201827   \n",
       "2                     1.440380               0.250894   \n",
       "3                     1.407457               0.115618   \n",
       "4                     1.226116               0.105015   \n",
       "...                        ...                    ...   \n",
       "1599995               1.564971               0.018557   \n",
       "1599996               1.066991               0.024285   \n",
       "1599997               1.177903               0.128303   \n",
       "1599998               1.098747               0.505184   \n",
       "1599999               1.120351               0.395579   \n",
       "\n",
       "         max vertices-point distance  min vertices-point distance  \\\n",
       "0                           0.884407                     0.509452   \n",
       "1                           0.794314                     0.651656   \n",
       "2                           0.854049                     0.552541   \n",
       "3                           0.733849                     0.540194   \n",
       "4                           0.700601                     0.527187   \n",
       "...                              ...                          ...   \n",
       "1599995                     0.901003                     0.571301   \n",
       "1599996                     0.604872                     0.476485   \n",
       "1599997                     0.631295                     0.570359   \n",
       "1599998                     0.608810                     0.491049   \n",
       "1599999                     0.567975                     0.479669   \n",
       "\n",
       "         distance to center  activity  particle type  image_id  \n",
       "0                  0.129601       1.0            1.0       0.0  \n",
       "1                  0.021338       1.0            1.0       0.0  \n",
       "2                  0.084406       1.0            1.0       0.0  \n",
       "3                  0.057512       1.0            1.0       0.0  \n",
       "4                  0.036320       1.0            1.0       0.0  \n",
       "...                     ...       ...            ...       ...  \n",
       "1599995            0.128033       0.0            0.0    1999.0  \n",
       "1599996            0.031195       0.0            0.0    1999.0  \n",
       "1599997            0.036724       0.0            0.0    1999.0  \n",
       "1599998            0.020731       0.0            0.0    1999.0  \n",
       "1599999            0.018331       0.0            0.0    1999.0  \n",
       "\n",
       "[1600000 rows x 17 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test\n",
    "probabilities = keras.activations.softmax(tf.convert_to_tensor(pred_test)).numpy()\n",
    "np.floor(probabilities[:,:,0]*2)\n",
    "len(probabilities[:][0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_df.activity,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [400, 400000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivity\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:649\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    642\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[0;32m    643\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_base.py:71\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m binary_metric(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m---> 71\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n\u001b[0;32m     73\u001b[0m y_score \u001b[38;5;241m=\u001b[39m check_array(y_score)\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [400, 400000]"
     ]
    }
   ],
   "source": [
    "roc_auc_score(np.floor(probabilities[:,:,0]*2),np.array(test_df.activity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
